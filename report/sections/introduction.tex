\section{Introduction}

Human activity recognition (HAR) plays an important role in applications such as 
surveillance, human–computer interaction, consumer devices, sports analytics, 
automotive safety, and healthcare monitoring. A central challenge is to represent 
human motion in a way that is compact, discriminative, and robust to variations 
in subjects, appearance, and viewpoint. Although deep learning approaches 
dominate modern HAR, classical motion-template methods remain attractive due to 
their interpretability and computational efficiency.

Motion History Images (MHI) and Motion Energy Images (MEI), introduced by 
Bobick and Davis~\cite{bobick2001}, transform video sequences into static 
templates that summarize where motion occurs and how it evolves over time. Their 
simplicity and ability to encode coarse action dynamics have made them 
foundational in early action-recognition pipelines. To illustrate the diversity 
of actions considered in this work, representative frames for all six activity 
classes are shown in Figure~\ref{fig:all-actions} 


Once constructed, these templates support the extraction of global shape-based 
features. Hu moments provide a compact, invariant descriptor widely used in 
classical vision tasks~\cite{ming2009hu}. More expressive local descriptors such 
as Histograms of Oriented Gradients (HOG)~\cite{dalal2005} capture fine spatial 
detail and highlight the trade-off between simplicity and representational 
richness—an important theme motivating this work.

This paper evaluates an MHI/MEI-based action-recognition system using three 
classical classifiers: Support Vector Machines (SVM)~\cite{cortes1995support}, 
$k$-Nearest Neighbors (KNN), and a lightweight Multi-Layer Perceptron (MLP). All 
models are trained on Hu-moment descriptors, enabling a controlled comparison 
under a low-dimensional, globally defined feature representation.

The system pipeline includes preprocessing, motion-template construction, 
feature extraction, classifier training, and evaluation. Our results highlight 
both the strengths and limitations of Hu-moment features: they effectively 
capture coarse motion patterns but struggle with fine distinctions among similar 
actions~\cite{bobick2001}. These findings, together with advances in 
spatiotemporal feature learning~\cite{klaser2008}, motivate future exploration of 
richer descriptors such as HOG, trajectory-based features, or learned CNN 
representations.



\begin{figure}[t]
    \centering

    % Row 1
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        % TODO: replace with your actual image path
        \includegraphics[width=\linewidth]{figures/final/walking-action.png}
        \caption{Walking}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        % TODO: replace with your actual image path
        \includegraphics[width=\linewidth]{figures/final/jogging-action.png}
        \caption{Jogging}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        % TODO: replace with your actual image path
        \includegraphics[width=\linewidth]{figures/final/running-action.png}
        \caption{Running}
    \end{subfigure}

    \vspace{2pt}

    % Row 2
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        % TODO: replace with your actual image path
        \includegraphics[width=\linewidth]{figures/final/boxing-action.png}
        \caption{Boxing}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        % TODO: replace with your actual image path
        \includegraphics[width=\linewidth]{figures/final/waving-action.png}
        \caption{Hand Waving}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\linewidth}
        \centering
        % TODO: replace with your actual image path
        \includegraphics[width=\linewidth]{figures/final/clapping-action.png}
        \caption{Hand Clapping}
    \end{subfigure}

    \caption{Representative frames for all six actions in the dataset .}
    \label{fig:all-actions}
\end{figure}
