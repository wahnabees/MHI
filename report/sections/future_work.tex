\section{Future Work}
\label{sec:future}

The results obtained using Hu moments across SVM, KNN, and MLP indicate that
the primary bottleneck lies in the limited expressive power of the features
rather than in the classifiers themselves. To overcome these constraints and
achieve higher recognition accuracy, several focused directions offer strong
potential for improvement.

\subsection{Richer Spatial Descriptors: HOG Features}

Since Hu moments capture only coarse global shape, a natural next step is to
explore more expressive handcrafted descriptors. Histogram of Oriented
Gradients (HOG) provides detailed local gradient information and has been shown
to be highly effective for human-related classification tasks~\cite{dalal2005}.
Incorporating HOG features into the same MHI/MEI pipeline would allow a direct
comparison against Hu moments and may significantly strengthen class
separability.

\subsection{Deep Feature Extraction Using CNNs}

Deep convolutional neural networks can learn high-level spatial representations
directly from image data. CNN-based feature extraction has proven highly
effective for action recognition when applied to appearance or motion cues,
including silhouette-style inputs and video frames~\cite{simonyan2014twostream}.
Applying such models to MHI/MEI templates—or directly to grayscale video
frames—could yield richer and more robust descriptors than handcrafted
features, allowing the model to learn discriminative motion cues without
relying solely on static global shape descriptors.

\subsection{Temporal Modeling Beyond Static Templates}

Although MHI and MEI encode motion compactly, they collapse the temporal
dimension into a single image. Future work could incorporate models that
preserve sequential information, such as LSTMs, GRUs, or temporal CNNs. These
approaches have shown strong performance in capturing temporal structure in
video-based action recognition~\cite{ji2013cnn}. Models of this
kind can operate on frame-level or clip-level features and capture motion
patterns that unfold over time, improving recognition for actions where
temporal ordering is essential.

Overall, these three directions—richer handcrafted descriptors, deep learned
features, and explicit temporal modeling—provide promising pathways for
advancing the performance of template-based human activity recognition systems.
