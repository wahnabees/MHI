\section{Improvements}
\label{sec:improvements}

This section describes the refinements made during experimentation that led to
observable improvements in classification accuracy when using Hu moment
features with SVM, KNN, and MLP. The focus here is not on comparing feature
types, but on understanding how adjustments to preprocessing, motion template
construction, and classifier configuration strengthened performance within the
Hu-based system.

\subsection{Effect of Motion Template Parameters}

Early experiments showed that the quality of the MHI and MEI templates had a
significant impact on downstream classification accuracy, consistent with
observations in earlier temporal-template work~\cite{bobick2001}. Small
adjustments to the motion threshold $\theta$ in the frame-differencing step and
the temporal decay parameter $\tau$ produced cleaner silhouettes and reduced
noise artifacts. Cleaned motion templates yielded Hu moment features that were
more consistent across sequences and subjects.

These refinements were particularly beneficial for SVM, which relies on
well-separated feature distributions to construct stable decision boundaries.
Reducing template noise improved class separability and produced measurable
gains in validation and test accuracy, consistent with findings on margin-based
methods in low-dimensional settings~\cite{cortes1995support}.

\subsection{Classifier-Level Refinements}

Each classifier required careful tuning to maximize performance with Hu
moments:

\begin{itemize}
    \item \textbf{SVM:} Adjusting the penalty parameter $C$ and kernel width
    $\gamma$ led to smoother margins and reduced overfitting, a common effect
    observed in SVM-based action classification~\cite{schuldt2004recognizing}.

    \item \textbf{KNN:} The number of neighbors $k$ had a strong impact on
    generalization. Lower values (e.g., $k=1$) memorized the training set but
    resulted in inconsistent test accuracy. Slightly higher $k$ stabilized
    predictions across classes, aligning with classical insights on the
    trade-off between noise sensitivity and neighborhood size~\cite{cover1967nearest}.

    \item \textbf{MLP:} Even though Hu moments are low-dimensional, tuning the
    hidden layer size and regularization strength helped prevent the network
    from overfitting to small variations in the feature space. Similar effects
    have been reported in early learning-based action-recognition systems
    where simple descriptors limit the benefits of neural models~\cite{yang2007evaluating}.
\end{itemize}

These model-level refinements showed that small hyperparameter adjustments
improved classifier stability even when the feature representation remained
unchanged.

\subsection{Evidence of Improved Hu-Based Classification}

Figure~\ref{fig:cm-improvements-hu} shows the confusion matrix of the
best-performing Hu-based model in this study. Compared to earlier runs, the
confusion matrix exhibits a more pronounced diagonal and fewer cross-class
confusions, indicating improved discriminability between activities. Such
improvements are consistent with findings that cleaner silhouettes and stable
feature extraction reduce ambiguity in template-based recognition
systems~\cite{bobick2001}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/metrics/cm_test_mlp.png}
    \caption{Confusion matrix of the best-performing classifier trained on Hu
    moments. The strengthened diagonal structure reflects improved class
    separability after preprocessing and parameter tuning.}
    \label{fig:cm-improvements-hu}
\end{figure}

\subsection{Summary of Improvements}

Across all refinements, the most impactful improvements within the Hu-based
pipeline were:
\begin{itemize}
    \item cleaner MHI/MEI templates achieved through tuned thresholds and decay parameters,
    \item classifier-specific hyperparameter adjustments (SVM margin tuning, KNN neighbor selection, MLP regularization),
    \item improved stability and consistency in Hu moment features extracted from motion templates.
\end{itemize}

These improvements enhanced the overall robustness of the Hu-based system,
even though the inherent simplicity of Hu descriptors sets an upper limit on
accuracy. Future work may explore richer descriptors such as HOG or deep
features to overcome these feature-level constraints.
